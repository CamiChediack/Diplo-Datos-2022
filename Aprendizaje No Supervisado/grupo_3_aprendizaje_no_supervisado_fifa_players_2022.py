# -*- coding: utf-8 -*-
"""Grupo 3 - Aprendizaje No Supervisado -  FIFA players 2022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1suAdjL-juennCso2Pv87re9I3wkOo3Ia

#Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones

##Aprendizaje No Supervisado

Edición 2022

##Grupo 3

#Consigna

Práctico entregable: 
Utilizar la base de jugadores “players_22.csv” disponible en la página de Kaggle https://www.kaggle.com/datasets/stefanoleone992/fifa-22-complete-player-dataset . Considerar que la base 2022 no tiene el mismo formato que la base vista en clase, a los nombres de las variables se les agregó una keyword para identificar a qué tipo de habilidad corresponde.

Con la nueva base, realizar un análisis análogo al que realizamos en el cursado de la materia con los datos FIFA2019. Realice comentarios en cada parte (verbose=True ;))

1. Análisis exploratorio de la base.
2. Evaluación visual  e intuitiva de a dos variables numéricas por vez.
3. Uso de dos técnicas de clustering: por ejemplo k-medias, DBSCAN, mezcla de Gaussianas y/o alguna jerárquica. Elección justificada de hiper-parámetros
4. Evaluación y Análisis de los clusters encontrados.
5. Pregunta: ¿Se realizó alguna normalización o escalado de la base? ¿Por qué ?
6. Uso de alguna transformación (proyección, Embedding) para visualizar los resultados y/o usarla como preprocesado para aplicar alguna técnica de clustering.
"""

from plotly.offline import init_notebook_mode, plot,iplot
import plotly.graph_objs as go
init_notebook_mode(connected=True)
import matplotlib.pyplot as plt
import plotly.tools as tls#visualization
import plotly.figure_factory as ff#visualization
from sklearn import (manifold, preprocessing, decomposition)

import numpy as np
import seaborn as sns
import pandas as pd
import itertools
import warnings
import io

"""## 1 Analisis Exploratorio de la Base

##Carga de la base de datos de Kaggle - FIFA Players 2022
"""

df=pd.read_csv('https://raw.githubusercontent.com/jmateoramos/AprendizajeNoSup-Grupo3-KaggleFifa22/main/players_22.csv') 
df.head(5)

"""La base de datos esta compuesta por 110 variables relacionadas no solo a las habilidades del jugador sino también al club donde pertenecen, su sueldo, valor del jugador, vencimiento de su contrato, número de camiseta, posición en la que juega y otros datos más personales como el nombre, la fecha de nacimiento, el peso y la altura. """

# Descripción de algunas variables numéricas.
print(df.describe())

"""##Analisis de la variable Overall"""

sns.displot(df.overall, aspect=2) 
plt.ticklabel_format(style='plain', axis='x')

"""La variable 'Overall' es una variable numérica discreta, así como las otras de desempeño según habilidad ('Crossing','Finishing', 'HeadingAccuracy', etc.) toma valores enteros entre 0 y 100.

## Análisis de las diferentes variables Skills
Ahora las habilidades estan asociadas con una cierta key que las junta en distintos grupos. Los datos que estan en estas columnas son numéricos dado que se refieren al puntaje que tiene cada jugador en cuanto a esa habilidad. Mientras más cercano a 100 más habilidoso es el jugador en esa habilidad específica.
"""

attacking_skills = df.columns[df.columns.str.contains('attacking')]
skills = df.columns[df.columns.str.contains('skill')]
skill_move =  df.columns[df.columns.str.contains('movement')]
power = df.columns[df.columns.str.contains('power')]
mentality = df.columns[df.columns.str.contains('mentality')]
defending = df.columns[df.columns.str.contains('defending_')]
gk = df.columns[df.columns.str.contains('goalkeeping')]
general = ['pace', 'shooting', 'passing','defending','dribbling', 'physic']

#Habilidades generales
df_g = df[general]
df_g

"""Realizamos una matriz de correlación entre las habilidades generales y vemos que algunas de ellas están bastante correlacionadas de forma positiva, como por ejemplo shooting y dribbling. Esto nos podría dar una pauta de que si un jugador tiene un alto puntaje en alguna de estas dos habilidades, muy probablemente también tenga un alto puntaje en la otra habilidad. 

Esto nos podría ayudar a elegir las habilidades que nos brinden información suficiente para separar los jugadores en distintas clases. 
"""

import matplotlib.pyplot as plt
corr_df = df_g.corr(method='pearson') 

sns.heatmap(corr_df)
plt.show()

# Hay jugadores que juegan en más de una posición y por eso se divide la columna "player_positions" en tres columnas distintas para poder analizar mejor esta variable
df[['Position 1', 'Position 2', 'Position 3']] =  df['player_positions'].str.split(',', expand = True)
df['Position 2'] = df['Position 2'].fillna('-')
df['Position 3'] = df['Position 3'].fillna('-')
df.head(10)

"""## Posiciones de los jugadores

### Defender
CB : Centre-back

RB : Right-back

LB : Left-back

RWB : Right-wing-back

LWB: Left-wing-back

### Midfielder

CM : Central-midfielder

CDM : Central-defensive-mildfielder

CAM : Central-attacking-midfielder

RM : Right-Midfielder

LM : Left-Midfielder

### Forward
RW : Right-winger

ST : Striker

CF : Central-forward

LW : Left-winger

### GK : Goalkeeper
"""

#Concatenamos todas las habilidades en una lista 
skills_ratings = [*attacking_skills, *skill_move, *power, *mentality, *defending, *gk, *skills, *general]

skills_dict = {}
for i in skills_ratings:
  skills_dict[i] = "median"

"""Se agrupa por las tres columnas de posiciones para ver la mediana del puntaje de cada habilidad según las distintas posiciones en las que juega un mismo jugador. """

# Se agrupa por posición y se calcula la mediana de cada habilidad 
df_position = df.groupby(['Position 1', 'Position 2', 'Position 3']).agg(skills_dict)
df_position = df_position.reset_index()
df_position

pos_vs_skill = df_position.drop(columns = ['Position 2', 'Position 3']) #Consideramos la primer columna como la posición "principal" y dado que esta contiene 
# información de las 15 posiciones se va a utilizar sólo esta columna. 
pos_vs_skill = pos_vs_skill.groupby('Position 1').agg(skills_dict).reset_index() #Reagrupamos y sacamos la mediana de cada habilidad
pos_vs_skill_1 = pos_vs_skill.melt('Position 1', value_vars = skills_ratings, var_name = 'skill') #Transponemos el dataframe
pos_vs_skill_1 = pos_vs_skill_1.sort_values('value', ascending = False).rename(columns = {'Position 1':'Position'})
pos_vs_skill_1

"""Luego obtenemos la mejor habilidad por posición, es decir, aquella habilidad con mayor puntaje para esa posición del jugador."""

pos = pos_vs_skill_1.groupby('Position').head(1) #El dataframe estaba ordenado de mayor a menor, por lo tanto nos devuelve el valor máximo.
pos

"""Como conclusión llegamos a que la mayoría de los jugadores que pertencen a un mismo grupo de posiciones comparten la mejor habilidad. Los delanteros: movement acceleration, los defensores: pace, los medio campistas: power stamina y el arquero (que es individual) tiene como mejor habilidad los reflejos. Podriamos considerar 4 clusters según el grupo de posiciones.

##Analisis de los jugadores usando las variables 'Overall' y 'Position'.

### Mejores jugadores por posición:
"""

best_players_per_position=df.iloc[df.groupby(df['Position 1'])['overall'].idxmax()][['Position 1','short_name','overall']]
best_players_per_position

"""### Cantidad de jugadores por posición."""

# Para contar la cantidad de jugadores en una misma posición se van a volver a juntar las tres columnas de Position para obtener el total.
# Esto no coincide con la cantidad de jugadores dado que hay algunos que juegan entres 2 o 3 posiciones distintas.
col1 = pd.DataFrame(df['Position 1'].str.strip().value_counts()).reset_index()
col2 = df[df['Position 2']!= '-']
col2 = pd.DataFrame(col2['Position 2'].str.strip().value_counts()).reset_index()
col3 = df[df['Position 3']!= '-']
col3 = pd.DataFrame(col3['Position 3'].str.strip().value_counts()).reset_index()

df_pos = col1.merge(col2, on = 'index', how = 'left').merge(col3, on = 'index', how = 'left')
df_pos = df_pos.rename(columns = {'index':'Position'})
df_pos[['Position 1', 'Position 2', 'Position 3']] = df_pos[['Position 1', 'Position 2', 'Position 3']].fillna(0).astype(int)
df_pos['Total'] = df_pos['Position 1']+df_pos['Position 2']+df_pos['Position 3']
df_pos.sort_values('Total').reset_index(drop=True)

"""En la posición que menos jugadores hay es en medio campista y defensores en las alas izquierda y derecha.

Mejores Promedios de Overall por equipos
"""

club_avg_overall=df.groupby("club_name")["overall"].mean().reset_index().sort_values("overall",ascending=False)
club_avg_overall.head(8)

"""*Reduccion del tamaño de la base, dejando solo los jugadores con valores de Overall mayor a 75, considerando variables de desempeño por habilidad (skills_ratings)*"""

n=10000 

df_n=df.loc[:n] #se reduce la base a los n primeros jugadores

df_n=df_n[(df_n['overall']>75)]

# Creamos la base con las variables de habilidad (skill_ratings)
skills_ratings = [*attacking_skills, *skill_move, *power, *mentality, *defending, *gk, *general, *skills] 
print(len(skills_ratings), 'variables numéricas de desempeño según habilidad')

"""*En orden de tener un valor de referencia maximo de overall para analizar gráficamente los datos y la ubicación de los demas jugadores, debemos agregar un nuevo registro que sera nuestro "Most Valuable Player - MVP"*"""

MVPDict={'short_name':'MVP','overall':99}

for skills in skills_ratings:
  MVPDict[skills]=99
#Agregamos ese "jugador" al dataframe 
df_n=df_n.append(MVPDict, ignore_index=True)

"""*Creamos la nueva base, solamente con las variables Skills*

Algunas habilidades de los arqueros no las tienen el resto de las posiciones, por lo tanto esos valores aparecen como nulls en el dataframe. Se podrían llenar esos nulls con valores cero que es el puntaje mínimo, dado que es como si no tuvieran esa habilidad que si tienen los arqueros.
"""

df_skills=df_n[skills_ratings]
df_skills = df_skills.fillna(0)

df_skills.head(4)

"""## 2. Evaluación visual  e intuitiva de a dos variables numéricas por vez.

Este gráfico que nos muestra la distribución de cada variable y realiza un scatter plot entre dos variables, nos va a servir para elegir las habilidades que mejor separen los jugadores en distintos clusters. Las variables que ya separan los puntos en distintos grupos van a ser las más adecuadas para realizar los clusters de las distintas posiciones.
"""

sns.pairplot(df_skills[skills_ratings[0:10]], diag_kind="kde").map_lower(sns.kdeplot, levels=4, color=".2")

# Este gráfico nos muestra la distribución de cada variable (gráficos de barras) y un scatter plot cuando compara dos variables a la vez.

sns.pairplot(df_skills[skills_ratings[11:21]], diag_kind="kde").map_lower(sns.kdeplot, levels=4, color=".2")

sns.pairplot(df_skills[skills_ratings[21:31]], diag_kind="kde").map_lower(sns.kdeplot, levels=4, color=".2")

sns.pairplot(df_skills[skills_ratings[32:42]], diag_kind="kde").map_lower(sns.kdeplot, levels=4, color=".2")

#buscamos identificar los mejores jugadores
bool_crack=df_n["overall"] > 90

#Elegimos dos variables, para esto elegimos dos números entre 0 y  de n_skills-1
skill_1=skills_ratings[14]
skill_2=skills_ratings[5]
print(f'Skill uno:{skill_1} \t Skill dos: {skill_2}')

"""Graficamos un scatter plot según las dos habilidades elegidas anteriormente, cada punto representa los puntajes que tiene cada jugador en cada una de estas habilidades."""

palette=['blue','green','navy','red','black','pink']  
graf1 = go.Scatter(x=df_skills[skill_1], y=df_skills[skill_2],
                           mode='markers',
                        text=df_n.loc[:,'club_name'], 
                           marker=dict(color=palette[1],size=3)
                           )

crack =go.Scatter(x=df_skills.loc[bool_crack,skill_1], y=df_skills.loc[bool_crack,skill_2],name='Mejores Jugadores',
                      text=df_n.loc[bool_crack,'short_name'],
                      textfont=dict(color='black'),
                      opacity=0.6,marker=dict(color=palette[2],size=3), mode='text') #Mejores jugadores que tiene un overall mayor a 90

data=[graf1,crack]

layout = go.Layout(xaxis=dict(title=skill_1),
                yaxis=dict(title=skill_2),
                autosize=False, width=1000,height=650)

fig = go.Figure(data=data, layout=layout)
fig.show(renderer="colab")

#BUSCAMOS A UN JUGADOR PARA PODER UBICARNOS MEJOR EN LOS GRÁFICOS
search_player=df_n["short_name"]=='K. Mbappé' 

bool_crack=df_n["overall"] > 78
bool_MVP=df_n["short_name"]=='MVP'

#Elegir dos números entre 0 y  de n_skills-1.
skill_1=skills_ratings[6] 
skill_2=skills_ratings[12] 
print(f'Skill uno: {skill_1} \t Skill dos: {skill_2}')

palette=['blue','green','navy','red','black','pink']  
data=[]

n_crack =go.Scatter(x=df_skills.loc[bool_crack,skill_1], y=df_skills.loc[bool_crack,skill_2],name='Crack',
                      text=df_n.loc[bool_crack,'short_name'],
                      textfont=dict(family='sans serif',size=15,color='black'),
                      opacity=0.9,marker=dict(color=palette[2],size=7),mode='markers')

n_MVP =go.Scatter(x=df_skills.loc[bool_MVP,skill_1], y=df_skills.loc[bool_MVP,skill_2],name='MVP',
                           textfont=dict(family='sans serif',size=20,color='black'),
                           opacity=0.6,marker=dict(color=palette[3],size=30),mode='markers')


search_player =go.Scatter(x=df_skills.loc[search_player,skill_1], y=df_skills.loc[search_player,skill_2],name='Searched player',
                           text=df_n.loc[search_player,'short_name'],
                            textfont=dict(family='sans serif',size=20,color='black'),
                           opacity=1,marker=dict(color=palette[5],size=40),mode='markers+text')

data=[n_crack,n_MVP,search_player]

layout = go.Layout(title="Fifa Players",titlefont=dict(size=20),
                xaxis=dict(title=skill_1),
                yaxis=dict(title=skill_2),
                autosize=False, width=1000,height=650)

fig = go.Figure(data=data, layout=layout)

fig.show(renderer="colab")

"""Vemos que MBappé esta muy cerca del *Most Valuable Player*.

## 3- Uso de dos técnicas de clustering: por ejemplo k-medias, DBSCAN, mezcla de Gaussianas y/o alguna jerárquica. Elección justificada de hiper-parámetros

#Clustering con K-medias
"""

from sklearn.cluster import KMeans,MeanShift
from sklearn import decomposition
# Número de clusters buscado
n_clust = 4 #Tenemos 4 grupos según las posiciones: los delanteros, los defensores, el arquero y los medio campistas. 

km = KMeans(n_clusters=n_clust)
km.fit(df_skills) #utiliza todas las habilidades: 42 dimensiones

# Etiquetas
clusters = km.labels_
df_clusters=df_n.copy()
df_clusters['kmeans_4'] = km.labels_ #clusters
print('Kmeans encontró: ', max(km.labels_)+1, 'clusters encontrados forzando la cantidad')
df_clusters.head(10)

bool_grosso=df_n["overall"] > 85 #Jugadores muy buenos.
bool_no_grosso=df_n["overall"]<=84 #Jugadores promedio.

#Eligo dos números entre 0 y  de n_skills-1
skill_1=skills_ratings[16] 
skill_2=skills_ratings[17] 
print(f'Skill uno: {skill_1} \t Skill dos: {skill_2}') #elegimos las variables según los gráficos del punto 2.

"""Graficamos los clusters en base a solo dos habilidades y con los nombres de los jugadores que son muy buenos. """

kmedia_clusters = go.Scatter(x=df_skills[skill_1], y=df_skills[skill_2],
                           mode='markers',
                        text=df_n.loc[:,'short_name'],
                           marker=dict(
                                size=5,
                                color = clusters.astype(np.float), #set color equal to a variable
                                showscale=False)
                           )

grosso =go.Scatter(x=df_skills.loc[bool_grosso,skill_1], y=df_skills.loc[bool_grosso,skill_2],name='Grosso',
                      text=df_n.loc[bool_grosso,'short_name'],
                      textfont=dict(family='sans serif',size=10,color='black'),
                      opacity=0.9,mode='text')

data=[kmedia_clusters,grosso]

layout = go.Layout(title="Clustering con K-medias ",
                xaxis=dict(title=skill_1),
                yaxis=dict(title=skill_2),
                autosize=False, width=1000,height=650)

fig = go.Figure(data=data, layout=layout)
fig.show(renderer="colab")

"""El cluster en la esquina izquierda inferior son los que pertenecen al grupo de arquero. El resto de los grupos se mezclan un poco pero se pueden separar y visualizar bastante bien los distintos clusters.

###Prueba: para elegir el hiperparámetro n_clusters, variando de 2 a 11 clusters
"""

puntajes = [KMeans(n_clusters=i).fit(df_skills).inertia_ for i in range(2,12)]

plt.plot(np.arange(2, 12), puntajes)
plt.xlabel('Número de Clusters')
plt.ylabel("Inercia")
plt.title("Inercia de k-Medias VS Número de Clusters")

"""Por lo visto en el gráfico, el primer codo se encuentra en n=4, como fue propuesto en el ejercicio.

#Clustering con DBSCAN
"""

from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans, DBSCAN
from scipy.spatial import distance
from sklearn.neighbors import NearestNeighbors

"""En DBSCAN se debe establecer el **epsilon**, definida como la distancia máxima entre dos muestras para ser consideradas vecinas entre ellas. No es la distancia  máxima entre las observaciones de cada cluster sino entre clusters para poder agrupar los datos en distintos conjuntos.

Una forma de encontrar el epsilon adecuado es realizando el gráfico a continuación.
"""

#FIND EPSILON
k = 2
data_nn = df_n.copy()[['attacking_volleys', 'attacking_short_passing']] #Vamos probando con distintas variables
# Calculate NN
nearest_neighbors = NearestNeighbors(n_neighbors=k)
neighbors = nearest_neighbors.fit(data_nn)
distances, indices = neighbors.kneighbors(data_nn)
distances = np.sort(distances, axis=0)

# Get distances
distances = distances[:,1]

i = np.arange(len(distances))

sns.lineplot(
    x = i, 
    y = distances
)

plt.xlabel("Points")
plt.ylabel("Distance")

"""Por lo visto en el gráfico se va a utilizar una distancia igual a 4, a partir de ella hay una mayor cantidad de puntos que se encuentran más cerca entre ellos para poder formar los distintos conjuntos."""

# Asignamos a cada posición el grupo al cual pertencen
Delantero=['RW', 'ST', 'CF', 'LW']
Mediocampista=['LM','CAM','CDM','RM','CM']
Defensor=['RB','CB','LB','RB','RWB','LWB']
Arquero=['GK']

def pos2(position):
    if position in Delantero:
        return 'Delantero'
    
    elif position in Mediocampista:
        return 'Mediocampista'
    
    elif position in Defensor:
        return 'Defensor'
    
    elif position in Arquero:
        return 'Arquero'
    
    else:
        return 'nan'

df_n["Position2"]=df_n["player_positions"].str.split(',').str[0].apply(lambda x: pos2(x))
df_n["Position2"].value_counts()

dict_replace = {'Delantero' : 0, 'Mediocampista' : 1, 'Defensor' : 2, 'Arquero' : 3} 
df_n['Position3'] = df_n['Position2']
df_n.replace({"Position3": dict_replace}, inplace=True) # Cambiamos el nombre del grupo al que pertencen por números. Esto nos puede ayudar para comparar
# la asignación del cluster con DBSCAN con una asignación manual para ver si el método asigno a los jugadores de los mismos grupos en el mismo cluster.

df_n['dbscan'] = DBSCAN(eps=4).fit_predict(df_n[['attacking_volleys', 'attacking_short_passing']])
plt.scatter(
    df_n['attacking_volleys'],
    df_n['attacking_short_passing'],
    c = df_n['dbscan'],
    alpha=0.5,
    cmap='rainbow' 
              )
plt.xlabel("attacking_volleys")
plt.ylabel("attacking_short_passing")

"""# 4-Evaluación y Análisis de los clusters encontrados.

Del gráfico anterior podemos identificar claramente los 4 clusters utilizando una distancia máxima entre samples de 4.

Las variableas analizadas fueron "attacking_volleys" y "attacking_short_passing"

A simple vista podemos identificar al grupo de los arqueros en la parte inferior del grafico, con valores medios y bajos de attacking_short_passing pero bajos de attacking_volleys.
Por otro lado el grupo de Delanteros se concentra en la parte derecha superior con altos valores de ambos.

El resto de las posiciones poseen valores más aleatorios dependiendo de cada jugador.

#5 ¿Se realizó alguna normalización o escalado de la base? ¿Por qué ?

Todos los valores del dataset que fueron utilizados para formar los clusters poseen valores entre 0 y 100 dado que son puntajes de las habilidades, por lo cual no es fue necesario realizar una normalización de la base al variar en el mismo rango. Todas las variables son de naturaleza discreta.

#6. Uso de alguna transformación (proyección, Embedding) para visualizar los resultados y/o usarla como preprocesado para aplicar alguna técnica de clustering.

Tomamos el dataframe con la cantidad reducida de jugaores y con un overall mayor a 75 y solo nos quedamos con las columnas con las habilidades para aplicar un Embedding. En este caso vamos a utilizar PCA.
"""

df_n = df_n[df_n['short_name']!= 'MVP'] #Quitamos el MVP que habiamos creado solo para ubicarnos en el gráfico
df_pca = df_n[skills_ratings]
df_pca = df_pca.fillna(1)

std_scale=preprocessing.StandardScaler().fit(df_pca) #Para utilizar PCA se deben escalar los datos dado que el ajuste de PCA los centra y luego calcula la matriz de dipersión
# si los datos no estan escalados alguna de ellas puede "tironear" el PCA hacia una dirección. En este caso podría no ser tan necesario dado que las variables varían en el mismo 
# rango.  
df_scaled=std_scale.transform(df_pca)
df_scaled

"""La idea de utilizar un embeddings no es solo poder reducir la dimensionalidad cuando nuestro problema tiene muchas dimensiones (como en este caso que son 42 habilidades, es un espacio de 42 dimensiones), sino que elegir aquellas características que mejor represente los datos para poder agruparlos mejor.

Los componentes principales son las características que minimizan el error de reconstruir los datos originales, es decir, son combinaciones lineales que minimizan la pérdida de información que surge de transformar tus datos originales. 
"""

pca=decomposition.PCA(n_components=7) 

pca.fit(df_scaled) 

# proporción de varianza
print('proporción de varianza por componente: ', pca.explained_variance_ratio_)
# proporción de varianza acumulada
print ('proporción de varianza por componente acumulada: ', pca.explained_variance_ratio_.cumsum())

df_projected=pca.transform(df_scaled)
print ('tamaño de los datos: ', df_projected.shape)

"""Las 7 componentes principales explica más del 90% de la variabilidad de los datos, mayor variabilidad significa mayor información y por ende menor pérdida de la misma. Se busca la dimensión donde los datos varían más. """

pcs=pca.components_
data=[]

for i, (x,y) in enumerate(zip(pcs[0,:],pcs[1,:])):
    graph=go.Scatter(x=[0,x],y=[0,y],text=df_skills.columns[i],
                     mode='lines+markers+text',textposition='top left',textfont=dict(family='sans serif',size=15))
    data.append(graph)

layout = go.Layout(title="ACP - Fifa Skills",titlefont=dict(size=20),
            xaxis=dict(title='Componente 1'),
            yaxis=dict(title='Componente 2'),
            autosize=False, width=1050,height=750,
            showlegend=False)

fig = go.Figure(data=data, layout=layout)
fig.show(renderer="colab")

"""Aplicamos K-Means con los componentes principales encontrados. Con este método estamos forzando al algoritmo a que encuentre la cantidad de clusters que le indiquemos. """

n_clust = 4

km = KMeans(n_clusters=n_clust)
km.fit(df_projected)
clusters = km.labels_

bool_crack=df_n["overall"] > 85
bool_no_crack=df_n["overall"]<86

# Graficamos
data = []
kmean_clusters = go.Scatter(x=df_projected[:,0], y=df_projected[:,1],
                           mode='markers',
                        text=df_n.loc[:,'short_name'],
                           marker=dict(
                                size=5,
                                color = clusters.astype(np.float), 
                                colorscale='sunsetdark',
                                showscale=False)
                           )

crack =go.Scatter(x=df_projected[bool_crack,0], y=df_projected[bool_crack,1],name='Top players',
                      text=df_n.loc[bool_crack,'short_name'],
                      textfont=dict(family='sans serif',size=10,color='black'),
                      opacity=0.9,mode='text')

data=[kmean_clusters, crack]

layout = go.Layout(title="Clustering K means ",titlefont=dict(size=20),
                xaxis=dict(title='Componente 1'),
                yaxis=dict(title='Componente 2'),
                autosize=False, width=1000,height=650)

fig = go.Figure(data=data, layout=layout)
fig.show(renderer="colab")

"""Ahora realizamos lo mismo pero con Mean-Shift. En este caso nosotros podemos establecer el parámetro bandwidth que es el que determina cuántos clusters se van a formar dado que define un entorno de cercanía a cada dato donde va actualizando los candidatos a centroides que van a ser las medias de los puntos pertenecientes a una región. """

ms = MeanShift(bandwidth = 3, bin_seeding=True)

ms.fit(df_projected)

clusters2 = ms.labels_
cluster_centers = ms.cluster_centers_

labels_unique = np.unique(clusters2)
n_clusters_ = len(labels_unique)

print("Cantidad de clusters encontrados por Mean Shift : %d" % n_clusters_)

data=[]
MeanShift_clusters = go.Scatter(x=df_projected[:,0], y=df_projected[:,1],
                           mode='markers',
                        text=df_n.loc[:,'short_name'],
                           marker=dict(
                                size=5,
                                color = clusters2.astype(np.float), #set color equal to a variable
                                colorscale='agsunset',
                                showscale=False)
                           )

crack =go.Scatter(x=df_projected[bool_crack,0], y=df_projected[bool_crack,1],name='Top players',
                      text=df_n.loc[bool_crack,'short_name'],
                      textfont=dict(family='sans serif',size=10,color='black'),
                      opacity=0.9,mode='text')

data=[MeanShift_clusters,crack]

layout = go.Layout(title="Mean Shift",titlefont=dict(size=20),
                xaxis=dict(title='Componente 1'),
                yaxis=dict(title='Componente 2'),
                autosize=False, width=1000,height=650)

fig = go.Figure(data=data, layout=layout)
fig.show(renderer="colab")

"""Se prueba obtener las dos componentes principales utilizando T-SNE, este embbeding tiene la misma finalidad que el PCA en cuanto a la reducción de dimensionalidad pero lo hace a partir de la reconstrucción de la distribución de probabilidad, por lo tanto a pares objetos que son similares son asignados con una alta probabilidad y los pares que son distintos una baja probabilidad. """

from sklearn.manifold import TSNE
df_embedded = TSNE(2).fit_transform(df_scaled)

n_clust = 4

km_tsne = KMeans(n_clusters=n_clust)
km_tsne.fit(df_embedded)
clusters_tsne = km_tsne.labels_

data = []
kmean_clusters = go.Scatter(x=df_embedded[:,0], y=df_embedded[:,1],
                           mode='markers',
                        text=df_n.loc[:,'short_name'],
                           marker=dict(
                                size=5,
                                color = clusters_tsne.astype(np.float), 
                                colorscale='agsunset',
                                showscale=False)
                           )

crack =go.Scatter(x=df_embedded[bool_crack,0], y=df_embedded[bool_crack,1],name='Top players',
                      text=df_n.loc[bool_crack,'short_name'],
                      textfont=dict(family='sans serif',size=10,color='black'),
                      opacity=0.9,mode='text')

data=[kmean_clusters, crack]

layout = go.Layout(title="Clustering K means ",titlefont=dict(size=20),
                xaxis=dict(title='Componente 1'),
                yaxis=dict(title='Componente 2'),
                autosize=False, width=1000,height=650)

fig = go.Figure(data=data, layout=layout)
fig.show(renderer="colab")

ms_tsne = MeanShift(bin_seeding=True) #Vamos a dejar que el mismo algoritmo estime el parámetro bandwidth

ms_tsne.fit(df_embedded)

clusters2 = ms_tsne.labels_
cluster_centers = ms_tsne.cluster_centers_

labels_unique = np.unique(clusters2)
n_clusters_ = len(labels_unique)

print("Cantidad de clusters encontrados por Mean Shift : %d" % n_clusters_)

data=[]
MeanShift_clusters = go.Scatter(x=df_embedded[:,0], y=df_embedded[:,1],
                           mode='markers',
                        text=df_n.loc[:,'short_name'],
                           marker=dict(
                                size=5,
                                color = clusters2.astype(np.float), #set color equal to a variable
                                colorscale='agsunset',
                                showscale=False)
                           )

crack =go.Scatter(x=df_embedded[bool_crack,0], y=df_embedded[bool_crack,1],name='Top players',
                      text=df_n.loc[bool_crack,'short_name'],
                      textfont=dict(family='sans serif',size=10,color='black'),
                      opacity=0.9,mode='text')

data=[MeanShift_clusters,crack]

layout = go.Layout(title="Mean Shift",titlefont=dict(size=20),
                xaxis=dict(title='Componente 1'),
                yaxis=dict(title='Componente 2'),
                autosize=False, width=1000,height=650)

fig = go.Figure(data=data, layout=layout)
fig.show(renderer="colab")

"""Dado que estamos buscando formar clusters según las posiciones de los jugadores es más apropiado utilizar T-SNE dado que reconstruye los datos originales en base a la distribución de probabilidad de las distintas características, agrupando por similitud. En los últimos dos gráficos donde se aplican los mismos métodos de clusterización pero al dataframe escalado y aplicando T-SNE vemos que hay separación más marcada entre los distintos grupos. 

En este último método vemos que el algoritmo por si solo encuentra 3 clusters en vez de 4 que son los que buscamos. Esto puede deberse a que algunos grupos de posiciones comparten ciertas habilidades que los hacen dificilmente separables entre ellos, el grupo que en todos los casos se separa muy bien del resto son los arqueros mientras que le hes difícil separar totalmente los delanteros, de los defensores y de los mediocampistas. Sin embargo aunque no estén separados podemos ver que los grupos estan bien delimitados en estos últimos gráficos. 
"""